---
title: "Day 8 - Multivariate data visualization"
output:
  html_document:
    highlight: pygments
    theme: readable
    toc: yes
    toc_float: yes
    code_folding: hide
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

# Objectives

* Demonstrate `lubridate` for working with dates in R and `ggplot2`
* Define correlation
* Review how to implement smoothing lines
* Generate and interpret scatterplot matricies and correlation heatmaps
* Introduce and generate parallel coordinate plots
* Identify methods for implementing three-dimensional graphs in R

```{r packages, cache = FALSE, message = FALSE}
library(tidyverse)
library(ggthemes)
library(knitr)
library(broom)
library(stringr)
library(modelr)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal())
```

# Working with dates in R

> For more details on the `lubridate` package, check out [*R for Data Science*](http://r4ds.had.co.nz/dates-and-times.html).

When importing data, date variables can be somewhat tricky to correctly store and utilize. In a spreadsheet, tabular format, dates by default will appear either as numeric (`20174018`) or string (`2016-04-18`, `April 18th, 2017`, etc.) columns. If you want to perform tasks such as: extracting and summarizing over individual components (year, month, day, etc.), we need to represent dates in a different, yet standardized, format.

`lubridate` is a tidyverse package that facilitates working with dates (and date-times) in R.

```{r lubridate}
library(lubridate)
```

## Formatting dates

When using `readr` to import data files, R will use `parse_date()` or `parse_datetime()` to try and format any columns it thinks contain dates or date-times. To manually format dates from strings, use the appropriate function combining `y`, `m`, and `d` in the proper order depending on the original format of the date:

```{r format-date}
ymd("2017-01-31")
mdy("January 31st, 2017")
dmy("31-Jan-2017")
```

## Extracting date components

Let's practice extracting components of dates using an example dataset. `flights-departed.csv` is a time series data file containing the daily number of departing commercial flights in the United States from 1988-2008.

```{r flights}
(flights <- read_csv("data/flights-departed.csv"))
```

We will use `ggplot2` to generate several graphs based on the data. The first is a simple line plot over time of the daily commercial flights. To build this, we don't need to modify `flights`:

```{r flights-over-time}
ggplot(flights, aes(date, value)) +
  geom_line() +
  labs(x = NULL,
       y = "Number of departing commercial flights")
```

But this is quite noisy. Instead, let's draw a line plot depicting commercial flights over a one-year period, with separate lines for each year in the data (1988, 1989, 1990, etc.). To do that, we need to create a new variable `year` which will serve as our grouping variable in `ggplot()`:

```{r flights-year}
(flights <- flights %>%
  mutate(year = year(date),
         yday = yday(date),
         # hack to label the x-axis with months
         days = dmy(format(date,"%d-%m-2016"))))

ggplot(flights, aes(days, value)) +
  geom_line(aes(group = year), alpha = .2) +
  geom_smooth(se = FALSE) +
  scale_x_date(labels = scales::date_format("%b")) +
  labs(x = NULL,
       y = "Number of departing commercial flights")
```

Or we could summarize the distribution of departing commercial flights by days in each month over the 20 year time period:

```{r flights-by-month}
(flights <- flights %>%
  mutate(month = month(date, label = TRUE)))

ggplot(flights, aes(month, value)) +
  geom_violin() +
  geom_boxplot(width = .1, outlier.shape = NA) +
  labs(x = NULL,
       y = "Number of departing commercial flights")
```

Hmmm, there seems to be an outlier in September. What's up with that?

Finally, we can generate a heatmap depicting the change over time of this data by creating a calendar-like visualization.^[A la [U.S. Commercial Flights, 1995-2008](http://mbostock.github.io/d3/talk/20111018/calendar.html).] In order do this, we need the following grammar for the graph:

* Layer
    * Data - flights
    * Mapping
        * $x$ - weekday (e.g. Sunday, Monday, Tuesday)
        * $y$ - week in month (e.g. first week, second week, third week)
        * Fill - `value` (number of departing flights)
    * Statistical transformation (stat) - `identity`
    * Geometric object (geom) - `geom_tile()`
    * Position adjustment (position) - none
* Scale
    * Fill - low and high-end colors (use shading to identify in-between values)
* Coordinate system - Cartesian coordinate plane
* Faceting - `facet_grid()` (year X month)

In order to generate this graph then, we need to create several new variables for `flights`:

* Year
* Month
* Weekday
* Week-in-month

We can use `lubridate` to directly generate three of those variables (we've already generated `year` and `month`):

```{r parse-components}
(flights <- flights %>%
  mutate(weekday = wday(date, label = TRUE)))
```

We use `label = TRUE` to generate factor labels for these values (January, February, March) instead of the numeric equivalent (1, 2, 3).

To generate the final week-in-month variable, we need to combine a few `lubridate` functions to get exactly what we want:

```{r parse-week-in-month}
(flights <- flights %>%
  # generate variables for week in the year (1-54) and the day in the year (1-366)
  mutate(week = week(date),
         yday = yday(date)) %>%
  # normalize to draw calendar correctly - wday should represent the number of days from the Sunday of the week containing January 1st, then adjust based on that
  group_by(year) %>%
  mutate(yday = yday + wday(date)[1] - 2,
         week = floor(yday / 7)) %>%
  group_by(year, month) %>%
  mutate(week_month = week - min(week) + 1))
```

Now that we have the data correctly formatted and all the components are extracted, we can draw the graph:

```{r heatmap, fig.asp = 2}
ggplot(flights, aes(weekday, week_month, fill = value)) +
  facet_grid(year ~ month) +
  geom_tile(color = "black") +
  scale_fill_continuous(low = "green", high = "red") +
  scale_x_discrete(labels = NULL) +
  scale_y_reverse(labels = NULL) +
  labs(title = "Domestic commercial flight activity",
       x = NULL,
       y = NULL,
       fill = "Number of departing flights") +
  theme_void() +
  theme(legend.position = "bottom")
```

Aha, now the outlier makes sense. In the days following the September 11th attacks, the United States grounded virtually all commercial air traffic.

# Smoothing lines

When examining multivariate continuous data, scatterplots are a quick and easy visualization to assess relationships. However if the data points become too densely clustered, interpreting the graph becomes difficult. Consider the `diamonds` dataset:

```{r diamonds-point}
p <- ggplot(diamonds, aes(carat, price)) +
  geom_point() +
  scale_y_continuous(labels = scales::dollar) +
  labs(x = "Carat size",
       y = "Price")
p
```

What is the relationship between carat size and price? It appears positive, but there are also a lot of densely packed data points in the middle of the graph. **Smoothing lines** are a method for summarizing the relationship between variables to capture important patterns by approximating the functional form of the relationship. The functional form can take on many shapes. For instance, a very common functional form is a **best-fit line**, also known as **ordinary least squares (OLS)** or **simple linear regression**. We can estimate the model directly using `lm()`, or we can directly plot the line by using `geom_smooth(method = "lm")`:

```{r diamonds-lm}
p +
  geom_smooth(method = "lm", se = FALSE)
```

The downside to a linear best-fit line is that it assumes the relationship between the variables is **additive** and **monotonic**. Therefore the summarized relationship between carat size and price seems wildly incorrect for diamonds with a carat size larger than 3. Instead we could use a [**generalized additive model**](http://cfss.uchicago.edu/persp007_nonlinear.html#generalized_additive_models) which allow for flexible, non-linear relationships between the variables while still implementing a basic regression approach:^[`geom_smooth()` automatically implements the `gam` method for datasets with greater than 1000 observations.]

```{r diamonds-gam}
p +
  geom_smooth(se = FALSE)
```

**Locally weighted scatterplot smoothing** (local regression, LOWESS, or LOESS) fits a separate non-linear function at each target point $x_0$ using only the nearby training observations. This method estimates a regression line based on localized subsets of the data, building up the global function $f$ point-by-point.

Here is an example of a local linear regression on the `ethanol` dataset in the `lattice` package:

```{r loess, echo = FALSE, warning = FALSE, message = FALSE}
library(lattice)

mod <- loess(NOx ~ E, ethanol, degree = 1, span = .75)
fit <- augment(mod)

mod0 <- loess(NOx ~ E, ethanol, degree = 0, span = .75)
mod1 <- loess(NOx ~ E, ethanol, degree = 1, span = .75)
mod2 <- loess(NOx ~ E, ethanol, degree = 2, span = .75)

fit_all <- ethanol %>%
  gather_predictions(mod0, mod1, mod2) %>%
  mutate(model = factor(model, levels = c("mod0", "mod1", "mod2"),
                        labels = c("Constant", "Linear", "Quadratic")))

ggplot(fit_all, aes(E, NOx)) +
  geom_point() +
  geom_line(aes(y = pred, color = model)) +
  labs(title = "Local linear regression",
       x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides in micrograms/J",
       color = "Regression")
```

The LOESS is built up point-by-point:

```{r loess_buildup, dependson="loess", fig.show = "animate", echo = FALSE, warning = FALSE, message = FALSE}
dat <- ethanol %>%
  inflate(center = unique(ethanol$E)) %>%
  mutate(dist = abs(E - center)) %>%
  filter(rank(dist) / n() <= .75) %>%
  mutate(weight = (1 - (dist / max(dist)) ^ 3) ^ 3)

library(gganimate)

p <- ggplot(dat, aes(E, NOx)) +
  geom_point(aes(alpha = weight, frame = center)) +
  geom_smooth(aes(group = center, frame = center, weight = weight), method = "lm", se = FALSE) +
  geom_vline(aes(xintercept = center, frame = center), lty = 2) +
  geom_line(aes(y = .fitted), data = fit, color = "red") +
  labs(x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides in micrograms/J")
gg_animate(p)
```

One important argument you can control with LOESS is the **span**, or how smooth the LOESS function will become. A larger span will result in a smoother curve, but may not be as accurate. A smaller span will be more local and wiggly, but improve our fit to the training data.

```{r loess_span, dependson="loess", fig.show = "animate", echo = FALSE, warning = FALSE, message = FALSE}
spans <- c(.25, .5, .75, 1)

# create loess fits, one for each span
fits <- data_frame(span = spans) %>%
  group_by(span) %>%
  do(augment(loess(NOx ~ E, ethanol, degree = 1, span = .$span)))

# calculate weights to reproduce this with local weighted fits
dat <- ethanol %>%
  inflate(span = spans, center = unique(ethanol$E)) %>%
  mutate(dist = abs(E - center)) %>%
  filter(rank(dist) / n() <= span) %>%
  mutate(weight = (1 - (dist / max(dist)) ^ 3) ^ 3)

# create faceted plot with changing points, local linear fits, and vertical lines,
# and constant hollow points and loess fit
p <- ggplot(dat, aes(E, NOx)) +
  geom_point(aes(alpha = weight, frame = center)) +
  geom_smooth(aes(group = center, frame = center, weight = weight), method = "lm", se = FALSE) +
  geom_vline(aes(xintercept = center, frame = center), lty = 2) +
  geom_point(shape = 1, data = ethanol, alpha = .25) +
  geom_line(aes(y = .fitted, frame = E, cumulative = TRUE), data = fits, color = "red") +
  facet_wrap(~span) +
  ylim(0, 5) +
  ggtitle("x0 = ") +
  labs(x = "Equivalence ratio",
       y = "Concentration of nitrogen oxides in micrograms/J")

gg_animate(p)
```

LOESS lines are best used for datasets with fewer than 1000 observations, otherwise the time and memory usage required to compute the line increases exponentially.

# Coefficient of correlation ($r$)

* Produces a measure of association, known as Pearson's $r$, that gauges the direction and strength of a relationship between two continuous variables
* Scales between $-1$ and $+1$
  * $-1$ -- perfect negative association between the variables
  * $+1$ -- perfect positive association between the variables
  * $0$ -- no relationship between the variables
* Unit-less measure - no matter what scale the variables fall on (e.g. turnout, education, income), the number will always fall between $-1$ and $+1$

```{r pearson-r}
r_plot <- function(r, n = 100){
  xy <- ecodist::corgen(len = n, r = r) %>%
    bind_cols
  
  ggplot(xy, aes(x, y)) +
    geom_point() +
    ggtitle(str_c("Pearson's r = ", r))
}

r <- c(.8, 0, -.8)

for(r in r){
  print(r_plot(r))
}
```

# Types of graphs to cover/demo

* Smoothing lines
    * Brief intro to the different `stat_smooth()` methods
* Scatterplot matricies
    * Heatmaps of correlation coefficients
* Parallel coordinate plots
* Contour plots
* 3D plots using plotly
    * Surface plots
    * Scatterplots
    * Line plots

# Session Info {.toc-ignore}

```{r cache = FALSE}
devtools::session_info()
```


